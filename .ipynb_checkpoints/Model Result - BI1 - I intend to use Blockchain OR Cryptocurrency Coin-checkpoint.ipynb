{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developed-keyboard",
   "metadata": {},
   "source": [
    "# Title: CORRELATION MODEL IN THE ADOPTION OF E-PAYMENT SERVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-nomination",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import skfeature.utility.entropy_estimators as ee\n",
    "\n",
    "from sklearn import svm, tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef, precision_recall_curve, roc_curve\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-manual",
   "metadata": {},
   "source": [
    "## Load Custom Made Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executed-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.CFS import *\n",
    "from Utilities.reliefF import *\n",
    "from Utilities.accuracy import *\n",
    "from Utilities.corr_matrix import *\n",
    "from Utilities.forward_selection import *\n",
    "from Utilities.backward_elimination import *\n",
    "\n",
    "from Visualization.model_graph import *\n",
    "from Visualization.network_graph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-frederick",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interior-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/E-payment Cryptocurrency Coin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-specification",
   "metadata": {},
   "source": [
    "## Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiovascular-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nominal(arr, term_arr):\n",
    "    tmp_dict = {val:ind for (ind, val) in enumerate(term_arr)}\n",
    "    return arr.map(lambda x : tmp_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-exploration",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "steady-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_fac_df = pd.DataFrame()\n",
    "\n",
    "column_arr = [\"Age\", \"Gender\", \"Marital Status\", \"Education Level\", \"Work Industry\", \"Work Position\"]\n",
    "\n",
    "for (ind, col_name) in enumerate(column_arr):\n",
    "    mod_fac_df[col_name] = df.iloc[:, 6 + ind]\n",
    "    \n",
    "# Replace Values in Work Industry\n",
    "mod_fac_df = mod_fac_df.replace(\"Baking / Finance\", \"Banking / Finance\")\n",
    "\n",
    "# Label Binarize all columns\n",
    "# Age\n",
    "mod_fac_df[\"Age\"] = convert_nominal(mod_fac_df[\"Age\"], [\"< 25 years\", \"26 - 40 years\", \"41 - 55 years\", \"above 55 years\"])\n",
    "\n",
    "# Gender\n",
    "mod_fac_df[\"Gender\"] = convert_nominal(mod_fac_df[\"Gender\"], [\"Male\", \"Female\"])\n",
    "\n",
    "# Marital Status\n",
    "mod_fac_df[\"Marital Status\"] = convert_nominal(mod_fac_df[\"Marital Status\"], [\"Single\", \"Married\", \"Other\"])\n",
    "\n",
    "# Education Level\n",
    "mod_fac_df[\"Education Level\"] = convert_nominal(mod_fac_df[\"Education Level\"], ['Primary school', 'Secondary/High school', 'College/university', 'Graduate school', 'Other'])\n",
    "\n",
    "# Work Industry\n",
    "mod_fac_df[\"Work Industry\"] = convert_nominal(mod_fac_df[\"Work Industry\"], ['Banking / Finance', 'Education', 'Healthcare', 'Manufacturing', 'Retail / Hypermarket', 'Other'])\n",
    "\n",
    "# Work Position\n",
    "mod_fac_df[\"Work Position\"] = convert_nominal(mod_fac_df[\"Work Position\"], ['Junior management', 'Middle management', 'Top management', 'Professional', 'Other'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finnish-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "utaut_fac_df = pd.DataFrame()\n",
    "\n",
    "column_arr = df.iloc[:, 28:].columns\n",
    "column_arr = [col.split(\": \")[0] for col in column_arr]\n",
    "\n",
    "for (ind, col_name) in enumerate(column_arr):\n",
    "    utaut_fac_df[col_name] = df.iloc[:, 28 + ind]\n",
    "    utaut_fac_df[col_name] = utaut_fac_df[col_name].map(lambda x : x - 1)\n",
    "    \n",
    "# Change Data Type to int\n",
    "utaut_fac_df = utaut_fac_df.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-registrar",
   "metadata": {},
   "source": [
    "## Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-supplier",
   "metadata": {},
   "source": [
    "### BI1: I intend to use Blockchain / Cryptocurrency Coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "professional-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_df = [mod_fac_df, utaut_fac_df.drop([\"BI1\", \"BI2\", \"BI3\", \"BI4\"], axis = 1)]\n",
    "\n",
    "df_X = pd.concat(arr_df, axis = 1)\n",
    "df_Y = utaut_fac_df.loc[:, \"BI1\"]\n",
    "\n",
    "# Convert Values to Int\n",
    "df_X = df_X.astype(int)\n",
    "df_Y = df_Y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-birmingham",
   "metadata": {},
   "source": [
    "#### Moderated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imposed-meter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_802c9_\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >Moderated Variable</th>        <th class=\"col_heading level0 col1\" >P-Value</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_802c9_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "                        <td id=\"T_802c9_row0_col1\" class=\"data row0 col1\" >0.014702</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_802c9_row1_col0\" class=\"data row1 col0\" >Work Position</td>\n",
       "                        <td id=\"T_802c9_row1_col1\" class=\"data row1 col1\" >0.002541</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b1152cd640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use SpearmanR as df_Y is an ordinal variable\n",
    "arr_list = []\n",
    "\n",
    "for col in mod_fac_df.columns:\n",
    "    corr, p_val = spearmanr(mod_fac_df.loc[:, col], df_Y)\n",
    "    if p_val <= 0.05:\n",
    "        arr_list.append((col, p_val))\n",
    "        \n",
    "tmp_df = pd.DataFrame(arr_list, columns = [\"Moderated Variable\", \"P-Value\"])\n",
    "\n",
    "tmp_df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-might",
   "metadata": {},
   "source": [
    "### CFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "future-graham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PE1', 'AT1', 'FC4', 'SE4', 'AX4', 'T1', 'T2', 'T4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_df = [mod_fac_df, utaut_fac_df.drop([\"BI1\", \"BI2\", \"BI3\", \"BI4\"], axis = 1)]\n",
    "\n",
    "tmp_X = pd.concat(arr_df, axis = 1)\n",
    "tmp_Y = df_Y\n",
    "\n",
    "feature_set = CFS(tmp_X, tmp_Y, spearmanr)\n",
    "\n",
    "feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-rainbow",
   "metadata": {},
   "source": [
    "### Weka Based Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imported-manner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'information_gain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7ed20456aea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Information Gain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_Y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0minfo_gain_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'information_gain' is not defined"
     ]
    }
   ],
   "source": [
    "arr_df = [mod_fac_df, utaut_fac_df.drop([\"BI1\", \"BI2\", \"BI3\", \"BI4\"], axis = 1)]\n",
    "\n",
    "tmp_X = pd.concat(arr_df, axis = 1)\n",
    "tmp_Y = df_Y\n",
    "\n",
    "tmp_df = pd.DataFrame()\n",
    "\n",
    "tmp_df[\"Factor\"] = tmp_X.columns\n",
    "\n",
    "# Chi Square\n",
    "corr, p_val = chi2(tmp_X, tmp_Y)\n",
    "tmp_df[\"Chi-Square\"] = corr\n",
    "tmp_df[\"Chi-Square (Rank)\"] = [*stats.rankdata(tmp_df[\"Chi-Square\"]).astype(int)]\n",
    "\n",
    "# Information Gain\n",
    "info_gain_arr = []\n",
    "\n",
    "for col in tmp_X.columns:\n",
    "    # Information Gain\n",
    "    val = information_gain(tmp_X.loc[:, col], tmp_Y) * -1\n",
    "    info_gain_arr.append(val)\n",
    "    \n",
    "tmp_df[\"Info Gain\"] = np.array(info_gain_arr) * -1\n",
    "tmp_df[\"Info Gain (Rank)\"] = [*stats.rankdata(tmp_df[\"Info Gain\"]).astype(int)]\n",
    "\n",
    "# ReliefF Algorithm\n",
    "tmp_df[\"ReliefF\"] = reliefF(tmp_X, tmp_Y)\n",
    "tmp_df[\"ReliefF (Rank)\"] = [*stats.rankdata(tmp_df[\"ReliefF\"]).astype(int)]\n",
    "\n",
    "tmp_df.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-entry",
   "metadata": {},
   "source": [
    "#### Remove Features that are not significant with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list = []\n",
    "\n",
    "# Get List of P_Values\n",
    "for col in df_X.columns:\n",
    "    corr, p_val = spearmanr(df_X.loc[:, col], df_Y)\n",
    "    # Threshold => Only append variables that have a p_value smaller than 0.05\n",
    "    if p_val <= 0.05:\n",
    "        arr_list.append((col, p_val))\n",
    "        \n",
    "# Sort Variables Ascending by P_val\n",
    "arr_list = sorted(arr_list, key = lambda x : x[1])\n",
    "\n",
    "arr_df = pd.DataFrame(arr_list, columns = [\"Variables\", \"P Value\"])\n",
    "\n",
    "arr_df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-state",
   "metadata": {},
   "source": [
    "### Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelObj:\n",
    "    def __init__(self, model, name, accuracy, clf_report, confusion_matrix, mcc):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.accuracy = accuracy\n",
    "        self.clf_report = clf_report\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.mcc = mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ModelObj(model, name, X, Y, class_arr):\n",
    "    # Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "    \n",
    "    # Train Model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get Y Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc_score = accuracy_score(y_pred, y_test) * 100.0\n",
    "\n",
    "    # Classification Report\n",
    "    tf_dict = { str(ind):val for ind, val in enumerate(class_arr)}\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict = True))\n",
    "    clf_report.rename(tf_dict, axis = 1, inplace=True)\n",
    "    clf_report = clf_report.T\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tf_dict = { ind:val for ind, val in enumerate(class_arr)}\n",
    "    confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "    confusion_matrix_df.rename(tf_dict, axis = 0, inplace=True)\n",
    "    confusion_matrix_df.rename(tf_dict, axis = 1, inplace=True)\n",
    "    \n",
    "    # Matthew Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    return ModelObj(model, name, acc_score, clf_report, confusion_matrix_df, mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-assault",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-purple",
   "metadata": {},
   "source": [
    "#### Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, arr_df[\"Variables\"]]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "name = \"Decision Tree\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-integrity",
   "metadata": {},
   "source": [
    "#### Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz(\n",
    "    model_dict[\"Decision Tree\"].model, \n",
    "    tmp_X, \n",
    "    tmp_Y, \n",
    "    feature_names = tmp_X.columns, \n",
    "    class_names = [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"],\n",
    "    fancy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-hypothesis",
   "metadata": {},
   "source": [
    "#### With Feature Selection (Wrapper Based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, feature_set]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "name = \"Decision Tree (CFS)\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-metro",
   "metadata": {},
   "source": [
    "### Random Forest (Bagging Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-norman",
   "metadata": {},
   "source": [
    "#### Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, arr_df[\"Variables\"]]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "name = \"Random Forest\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-thanksgiving",
   "metadata": {},
   "source": [
    "#### With Feature Selection (CFS Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, feature_set]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "name = \"Random Forest (CFS)\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-stone",
   "metadata": {},
   "source": [
    "### XGBoost (Boosting Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-rendering",
   "metadata": {},
   "source": [
    "#### Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, arr_df[\"Variables\"]]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = XGBClassifier(eval_metric='error', use_label_encoder=False)\n",
    "name = \"XGBoost\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-doctrine",
   "metadata": {},
   "source": [
    "#### With Feature Selection (CFS Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X = df_X.loc[:, feature_set]\n",
    "tmp_Y = df_Y\n",
    "\n",
    "model = XGBClassifier(eval_metric='error', use_label_encoder=False)\n",
    "name = \"XGBoost (CFS)\"\n",
    "\n",
    "acc_score = get_acc_score_kcv(tmp_X, tmp_Y, model)\n",
    "\n",
    "print(f\"Accuracy Score: {round(acc_score, 2)}%\")\n",
    "\n",
    "model_dict[name] = create_ModelObj(model, name, tmp_X, tmp_Y, [\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-infrastructure",
   "metadata": {},
   "source": [
    "## Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X.loc[:, arr_df[\"Variables\"]], df_Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-honor",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_arr = [(name, model_dict[name].clf_report, model_dict[name].mcc) for name in model_dict]\n",
    "cmp_result_tbl(m_arr, \"weighted avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-thumb",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report_arr = [(name, model_dict[name].clf_report) for name in model_dict]\n",
    "tmp_df = get_df_type(clf_report_arr, \"Precision\")\n",
    "pfr_graph(tmp_df, \"Model\", \"Score\", \"Precison Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-letters",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report_arr = [(name, model_dict[name].clf_report) for name in model_dict]\n",
    "tmp_df = get_df_type(clf_report_arr, \"Recall\")\n",
    "pfr_graph(tmp_df, \"Model\", \"Score\", \"Recall Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-penguin",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report_arr = [(name, model_dict[name].clf_report) for name in model_dict]\n",
    "tmp_df = get_df_type(clf_report_arr, \"F1-Score\")\n",
    "pfr_graph(tmp_df, \"Model\", \"Score\", \"F1-Score Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-feeding",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = [(key, model_dict[key].accuracy) for key in model_dict]\n",
    "acc_graph(acc_arr, \"Accuracy Score Comparison\", \"Accuracy Score\", \"Types of Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
